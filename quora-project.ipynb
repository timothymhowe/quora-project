{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Identifying Duplicate Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Over 100 million people visit Quora every month, so it's no surprise that many people ask similar (or the same) questions. Various questions with the same intent can cause people to spend extra time searching for the best answer to their question, and results in members answering multiple versions of the same question. Quora uses random forest to identify duplicated questions to provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n",
    "Follow the steps outlined below to build the appropriate classifier model. \n",
    "\n",
    "\n",
    "Steps:\n",
    "- Download data\n",
    "- Exploration\n",
    "- Cleaning\n",
    "- Feature Engineering\n",
    "- Modeling\n",
    "\n",
    "By the end of this project you should have **a presentation that describes the model you built** and its **performance**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Note\n",
    "There is no designated test.csv file. The train.csv file is the entire dataset. Part of the data in the train.csv file should be set aside to act as the final testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.csv\",index_col=0)\n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "\n",
    "y = df['is_duplicate']\n",
    "X = df.drop('is_duplicate',axis=1)\n",
    "\n",
    "y.index = X.index\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=.75,random_state=1923)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "count    303215.000000\nmean         10.938948\nstd           5.434235\nmin           1.000000\n25%           7.000000\n50%          10.000000\n75%          13.000000\nmax         125.000000\nName: question1, dtype: float64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "X_train['question1'].map(lambda x: len(x.split(\" \"))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1440x360 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, (ax1,ax2) = plt.subplots(2,1,sharex='all',figsize=(20,5))\n",
    "ax1.boxplot(X_train['question1'].map(lambda x: len(x.split())), vert=False,);\n",
    "ax2.boxplot(X_train['question2'].map(lambda x: len(x.split())),vert=False);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1440x360 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, (ax1,ax2) = plt.subplots(2,1,sharex=True,figsize=(20,5));\n",
    "ax1.hist(X_train['question1'].map(lambda x: len(x.split())),bins=100, log=True);\n",
    "ax2.hist(X_train['question2'].map(lambda x: len(x.split())),bins=100, log=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "ugh = df['qid1'].to_numpy()\n",
    "lenlen = len(set(ugh.tolist() + df['qid2'].to_numpy().tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                qid1           qid2   is_duplicate\ncount  404287.000000  404287.000000  404287.000000\nmean   217243.151093  220955.212082       0.369201\nstd    157751.614317  159903.168488       0.482589\nmin         1.000000       2.000000       0.000000\n25%     74436.500000   74726.500000       0.000000\n50%    192181.000000  197053.000000       0.000000\n75%    346573.000000  354692.000000       1.000000\nmax    537932.000000  537933.000000       1.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>is_duplicate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>404287.000000</td>\n      <td>404287.000000</td>\n      <td>404287.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>217243.151093</td>\n      <td>220955.212082</td>\n      <td>0.369201</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>157751.614317</td>\n      <td>159903.168488</td>\n      <td>0.482589</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>74436.500000</td>\n      <td>74726.500000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>192181.000000</td>\n      <td>197053.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>346573.000000</td>\n      <td>354692.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>537932.000000</td>\n      <td>537933.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290652\n",
      "299362\n"
     ]
    }
   ],
   "source": [
    "print(len(set(df['qid1'])))\n",
    "\n",
    "print(len(set(df['qid2'])))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404287\n"
     ]
    }
   ],
   "source": [
    "print(len(df['qid1']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cleaning\n",
    "\n",
    "- Tokenization\n",
    "- Stopwords cleaning\n",
    "- Removing punctuation\n",
    "- Normalizing\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Word Tokenization\n",
    "from nltk.tokenize import NLTKWordTokenizer\n",
    "\n",
    "tokenizer = NLTKWordTokenizer()\n",
    "X_train['question1_tokens'] = X_train['question1'].map(tokenizer.tokenize)\n",
    "X_train['question2_tokens'] = X_train['question2'].map(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "X_train2 = X_train\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "X_train2['question1_tokens'] = X_train['question1_tokens'].map(lambda xs : [x for x in xs if not x.lower() in sw])\n",
    "X_train2['question2_tokens'] = X_train['question2_tokens'].map(lambda xs : [x for x in xs if not x.lower() in sw])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "from string import punctuation as punc_list\n",
    "X_train2['question1_tokens'] = X_train2['question1_tokens'].map(lambda xs : [x for x in xs if not x in punc_list])\n",
    "X_train2['question2_tokens'] = X_train2['question2_tokens'].map(lambda xs : [x for x in xs if not x in punc_list])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Use porter's algorithm to get stems, and convert to lowercase!\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "X_train2['question1_tokens'] = X_train2['question1_tokens'].map(lambda xs : [stemmer.stem(x, to_lowercase=True) for x in xs])\n",
    "X_train2['question2_tokens'] = X_train2['question2_tokens'].map(lambda xs : [stemmer.stem(x,to_lowercase=True) for x in xs])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "          qid1                                   question1_tokens    qid2  \\\nid                                                                          \n20369    38434                             [3, main, compon, cpu]   38435   \n128211  206240    [posit, neg, implic, could, come, time, travel]  206241   \n207728  311500     [feel, see, time, 9:11, everytim, look, clock]  311501   \n327614   14244           [one, correct, lunch, lunch, ate, lunch]   34902   \n206680  310177                                      [read, quora]  120989   \n...        ...                                                ...     ...   \n363276  227992  [place, india, everi, indian, visit, least, life]  385352   \n308849   26089                                [live, happi, life]  260208   \n151699  238470                      [factori, reset, ipod, touch]  238471   \n328037  172373                     [canon, lens, get, 750d, 760d]  454544   \n254671   61668           [common, trait, highli, intellig, peopl]  369445   \n\n                                         question2_tokens  \nid                                                         \n20369                                 [main, compon, cpu]  \n128211                           [dark, upper, lip, turn]  \n207728                   [mean, alway, see, 20:20, clock]  \n327614                               [determin, use, etc]  \n206680                                [peopl, use, quora]  \n...                                                   ...  \n363276               [place, everyon, visit, india, life]  \n308849       [person, live, success, happi, life, invest]  \n151699             [perform, factori, reset, ipod, touch]  \n328037  [take, good, photo, use, canon, eo, 750d, also...  \n254671  [peopl, high, intellig, look, peopl, normal, i...  \n\n[303215 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid1</th>\n      <th>question1_tokens</th>\n      <th>qid2</th>\n      <th>question2_tokens</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20369</th>\n      <td>38434</td>\n      <td>[3, main, compon, cpu]</td>\n      <td>38435</td>\n      <td>[main, compon, cpu]</td>\n    </tr>\n    <tr>\n      <th>128211</th>\n      <td>206240</td>\n      <td>[posit, neg, implic, could, come, time, travel]</td>\n      <td>206241</td>\n      <td>[dark, upper, lip, turn]</td>\n    </tr>\n    <tr>\n      <th>207728</th>\n      <td>311500</td>\n      <td>[feel, see, time, 9:11, everytim, look, clock]</td>\n      <td>311501</td>\n      <td>[mean, alway, see, 20:20, clock]</td>\n    </tr>\n    <tr>\n      <th>327614</th>\n      <td>14244</td>\n      <td>[one, correct, lunch, lunch, ate, lunch]</td>\n      <td>34902</td>\n      <td>[determin, use, etc]</td>\n    </tr>\n    <tr>\n      <th>206680</th>\n      <td>310177</td>\n      <td>[read, quora]</td>\n      <td>120989</td>\n      <td>[peopl, use, quora]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>363276</th>\n      <td>227992</td>\n      <td>[place, india, everi, indian, visit, least, life]</td>\n      <td>385352</td>\n      <td>[place, everyon, visit, india, life]</td>\n    </tr>\n    <tr>\n      <th>308849</th>\n      <td>26089</td>\n      <td>[live, happi, life]</td>\n      <td>260208</td>\n      <td>[person, live, success, happi, life, invest]</td>\n    </tr>\n    <tr>\n      <th>151699</th>\n      <td>238470</td>\n      <td>[factori, reset, ipod, touch]</td>\n      <td>238471</td>\n      <td>[perform, factori, reset, ipod, touch]</td>\n    </tr>\n    <tr>\n      <th>328037</th>\n      <td>172373</td>\n      <td>[canon, lens, get, 750d, 760d]</td>\n      <td>454544</td>\n      <td>[take, good, photo, use, canon, eo, 750d, also...</td>\n    </tr>\n    <tr>\n      <th>254671</th>\n      <td>61668</td>\n      <td>[common, trait, highli, intellig, peopl]</td>\n      <td>369445</td>\n      <td>[peopl, high, intellig, look, peopl, normal, i...</td>\n    </tr>\n  </tbody>\n</table>\n<p>303215 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train2[['qid1','question1_tokens','qid2','question2_tokens']]\n",
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "class QuoraCleaner(BaseEstimator,TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "\n",
    "    def transform(self,X,y=None):\n",
    "        # tokenize strings\n",
    "        from nltk.tokenize import NLTKWordTokenizer\n",
    "        tokenizer = NLTKWordTokenizer()\n",
    "        X['question1_tokens'] = X['question1'].map(tokenizer.tokenize)\n",
    "        X['question2_tokens'] = X['question2'].map(tokenizer.tokenize)\n",
    "\n",
    "        # remove stopwords\n",
    "        from nltk.corpus import stopwords\n",
    "        sw = stopwords.words('english')\n",
    "        X['question1_tokens'] = X['question1_tokens'].map(lambda xs : [x for x in xs if not x.lower() in sw])\n",
    "        X['question2_tokens'] = X['question2_tokens'].map(lambda xs : [x for x in xs if not x.lower() in sw])\n",
    "\n",
    "        # remove punctuation\n",
    "        from string import punctuation as punc_list\n",
    "        X['question1_tokens'] = X['question1_tokens'].map(lambda xs : [x for x in xs if not x in punc_list])\n",
    "        X['question2_tokens'] = X['question2_tokens'].map(lambda xs : [x for x in xs if not x in punc_list])\n",
    "\n",
    "        # Use porter's algorithm to get stems.\n",
    "        from nltk.stem import PorterStemmer\n",
    "        stemmer = PorterStemmer()\n",
    "        X['question1_tokens'] = X['question1_tokens'].map(lambda xs : [stemmer.stem(x, to_lowercase=True) for x in xs])\n",
    "        X['question2_tokens'] = X['question2_tokens'].map(lambda xs : [stemmer.stem(x,to_lowercase=True) for x in xs])\n",
    "\n",
    "        X = X[['qid1','question1_tokens','qid2','question2_tokens']]\n",
    "\n",
    "        return X\n",
    "X_test = QuoraCleaner().transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Feature Engineering\n",
    "\n",
    "- tf-idf\n",
    "- word2vec\n",
    "- word count\n",
    "- number of the same words in both questions\n",
    "- ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# get uniq list of all documents for vectorization\n",
    "import numpy as np\n",
    "\n",
    "temp1 = X_train[['qid1','question1_tokens']].groupby('qid1').first()\n",
    "temp1 = temp1.reset_index()\n",
    "\n",
    "temp1.columns = ['qid','tokens']\n",
    "\n",
    "temp2 = X_train[['qid2','question2_tokens']].groupby('qid2').first()\n",
    "temp2 = temp2.reset_index()\n",
    "temp2.columns = ['qid','tokens']\n",
    "uniqs = pd.concat([temp1,temp2],axis=0).groupby('qid').first()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec_count = CountVectorizer(preprocessor=' '.join)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tran_tfidf = TfidfTransformer()\n",
    "\n",
    "# tfidf_pipe = Pipeline([('count',vec_count),('tfidf',tran_tfidf)]).fit(uniqs['tokens'])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec_tfidf = TfidfVectorizer(preprocessor=' '.join,ngram_range=(1,3)).fit(uniqs['tokens'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "q1_tfidf = vec_tfidf.transform(X_train['question1_tokens'])\n",
    "q2_tfidf = vec_tfidf.transform(X_train['question2_tokens'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thowe\\AppData\\Local\\Temp\\ipykernel_7892\\992028720.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['tfidf_cos_distances'] = paired_cosine_distances(q1_tfidf,q2_tfidf)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import paired_cosine_distances\n",
    "X_train['tfidf_cos_distances'] = paired_cosine_distances(q1_tfidf,q2_tfidf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.similarities import WordEmbeddingSimilarityIndex, SparseTermSimilarityMatrix, SoftCosineSimilarity\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "w2v = word2vec.load_word2vec_format('C:/Users/thowe/.word2vec/GoogleNews-vectors-negative300.bin',binary=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "q1_num_words = X_train['question1_tokens'].apply(len)\n",
    "q2_num_words = X_train['question2_tokens'].apply(len)\n",
    "X_train['difference_in_number_of_words'] = q1_num_words - q2_num_words\n",
    "\n",
    "def cosine_sim(vecA, vecB):\n",
    "    \"\"\"\n",
    "    Find the cosine similarity distance between two vectors.\n",
    "\n",
    "    :param vecA: the first vector for the similarity calculation\n",
    "    :param vecB: the second vector for the similarity calculation\n",
    "    :return: The cosine similarity of the two vectors\n",
    "    \"\"\"\n",
    "    cos_sim = np.dot(vecA, vecB) / (np.linalg.norm(vecA) * np.linalg.norm(vecB))\n",
    "    if np.isnan(np.sum(cos_sim)):\n",
    "        return 0\n",
    "    return cos_sim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [27]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m vocab \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(vocab))\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mlen\u001B[39m(vocab)\n\u001B[1;32m----> 5\u001B[0m w2v_stems \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw2v\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex_to_key\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapplymap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstemmer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstem\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\pandas\\core\\frame.py:8930\u001B[0m, in \u001B[0;36mDataFrame.applymap\u001B[1;34m(self, func, na_action, **kwargs)\u001B[0m\n\u001B[0;32m   8927\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer(x, func, ignore_na\u001B[38;5;241m=\u001B[39mignore_na)\n\u001B[0;32m   8928\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer(x\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values, func, ignore_na\u001B[38;5;241m=\u001B[39mignore_na)\n\u001B[1;32m-> 8930\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43minfer\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplymap\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\pandas\\core\\frame.py:8845\u001B[0m, in \u001B[0;36mDataFrame.apply\u001B[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001B[0m\n\u001B[0;32m   8834\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapply\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[0;32m   8836\u001B[0m op \u001B[38;5;241m=\u001B[39m frame_apply(\n\u001B[0;32m   8837\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   8838\u001B[0m     func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   8843\u001B[0m     kwargs\u001B[38;5;241m=\u001B[39mkwargs,\n\u001B[0;32m   8844\u001B[0m )\n\u001B[1;32m-> 8845\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapply\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\pandas\\core\\apply.py:733\u001B[0m, in \u001B[0;36mFrameApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    730\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw:\n\u001B[0;32m    731\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_raw()\n\u001B[1;32m--> 733\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\pandas\\core\\apply.py:857\u001B[0m, in \u001B[0;36mFrameApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    856\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_standard\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 857\u001B[0m     results, res_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_series_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;66;03m# wrap results\u001B[39;00m\n\u001B[0;32m    860\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrap_results(results, res_index)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\pandas\\core\\apply.py:873\u001B[0m, in \u001B[0;36mFrameApply.apply_series_generator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    870\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode.chained_assignment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(series_gen):\n\u001B[0;32m    872\u001B[0m         \u001B[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001B[39;00m\n\u001B[1;32m--> 873\u001B[0m         results[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    874\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(results[i], ABCSeries):\n\u001B[0;32m    875\u001B[0m             \u001B[38;5;66;03m# If we have a view on v, we need to make a copy because\u001B[39;00m\n\u001B[0;32m    876\u001B[0m             \u001B[38;5;66;03m#  series_generator will swap out the underlying data\u001B[39;00m\n\u001B[0;32m    877\u001B[0m             results[i] \u001B[38;5;241m=\u001B[39m results[i]\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\pandas\\core\\frame.py:8928\u001B[0m, in \u001B[0;36mDataFrame.applymap.<locals>.infer\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m   8926\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mempty:\n\u001B[0;32m   8927\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer(x, func, ignore_na\u001B[38;5;241m=\u001B[39mignore_na)\n\u001B[1;32m-> 8928\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mobject\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_na\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_na\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\nltk\\stem\\porter.py:674\u001B[0m, in \u001B[0;36mPorterStemmer.stem\u001B[1;34m(self, word, to_lowercase)\u001B[0m\n\u001B[0;32m    672\u001B[0m stem \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step2(stem)\n\u001B[0;32m    673\u001B[0m stem \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step3(stem)\n\u001B[1;32m--> 674\u001B[0m stem \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_step4\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstem\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    675\u001B[0m stem \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step5a(stem)\n\u001B[0;32m    676\u001B[0m stem \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step5b(stem)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\nltk\\stem\\porter.py:573\u001B[0m, in \u001B[0;36mPorterStemmer._step4\u001B[1;34m(self, word)\u001B[0m\n\u001B[0;32m    544\u001B[0m \u001B[38;5;124;03m\"\"\"Implements Step 4 from \"An algorithm for suffix stripping\"\u001B[39;00m\n\u001B[0;32m    545\u001B[0m \n\u001B[0;32m    546\u001B[0m \u001B[38;5;124;03mStep 4\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    569\u001B[0m \u001B[38;5;124;03mtidying up.\u001B[39;00m\n\u001B[0;32m    570\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    571\u001B[0m measure_gt_1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m stem: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_measure(stem) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 573\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply_rule_list\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    574\u001B[0m \u001B[43m    \u001B[49m\u001B[43mword\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    575\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\n\u001B[0;32m    576\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mal\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    577\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mance\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    578\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mence\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    579\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mer\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    580\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mic\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    581\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mible\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mant\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    584\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mement\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    585\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mment\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    586\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43ment\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    587\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# (m>1 and (*S or *T)) ION ->\u001B[39;49;00m\n\u001B[0;32m    588\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    589\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mion\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    590\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    591\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstem\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_measure\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstem\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstem\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43ms\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    592\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    593\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mou\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    594\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mism\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    595\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mate\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    596\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43miti\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    597\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mous\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    598\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mive\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    599\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeasure_gt_1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    600\u001B[0m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    601\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\nltk\\stem\\porter.py:259\u001B[0m, in \u001B[0;36mPorterStemmer._apply_rule_list\u001B[1;34m(self, word, rules)\u001B[0m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m rule \u001B[38;5;129;01min\u001B[39;00m rules:\n\u001B[0;32m    258\u001B[0m     suffix, replacement, condition \u001B[38;5;241m=\u001B[39m rule\n\u001B[1;32m--> 259\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43msuffix\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m*d\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ends_double_consonant(word):\n\u001B[0;32m    260\u001B[0m         stem \u001B[38;5;241m=\u001B[39m word[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m]\n\u001B[0;32m    261\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m condition \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m condition(stem):\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# keys = w2v.index_to_key\n",
    "vocab = np.concatenate(list(uniqs['tokens']))\n",
    "vocab = list(set(vocab))\n",
    "len(vocab)\n",
    "# w2v_stems = pd.DataFrame(w2v.index_to_key).applymap(stemmer.stem)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "84280"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   tokens\nqid                                                      \n3                  [stori, kohinoor, koh-i-noor, diamond]\n4       [would, happen, indian, govern, stole, kohinoo...\n5           [increas, speed, internet, connect, use, vpn]\n6                    [internet, speed, increas, hack, dn]\n7                                    [mental, lone, solv]\n...                                                   ...\n537925      [cpu, upgrad, 2016, appl, macbook, pro, mean]\n537926                           [jainism, say, homosexu]\n537927                      [jainism, say, gay, homosexu]\n537928                                        [one, coin]\n537929                                         ['s, coin]\n\n[427020 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tokens</th>\n    </tr>\n    <tr>\n      <th>qid</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>[stori, kohinoor, koh-i-noor, diamond]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[would, happen, indian, govern, stole, kohinoo...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[increas, speed, internet, connect, use, vpn]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[internet, speed, increas, hack, dn]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[mental, lone, solv]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>537925</th>\n      <td>[cpu, upgrad, 2016, appl, macbook, pro, mean]</td>\n    </tr>\n    <tr>\n      <th>537926</th>\n      <td>[jainism, say, homosexu]</td>\n    </tr>\n    <tr>\n      <th>537927</th>\n      <td>[jainism, say, gay, homosexu]</td>\n    </tr>\n    <tr>\n      <th>537928</th>\n      <td>[one, coin]</td>\n    </tr>\n    <tr>\n      <th>537929</th>\n      <td>['s, coin]</td>\n    </tr>\n  </tbody>\n</table>\n<p>427020 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embed_dict = {}\n",
    "\n",
    "stem_dict = []\n",
    "for index, word in enumerate(vocab):\n",
    "    try:\n",
    "        idx_dict[index] = w2v[word]\n",
    "    except:\n",
    "        pass\n",
    "del w2v"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keys=embed_dict.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# documents = [TaggedDocument(doc, tags=[i]) for i, doc in uniqs.iterrows()]\n",
    "documents2 = [TaggedDocument(*doc, tags=[i]) for i, doc in uniqs.iterrows()]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents2, vector_size=50, window=5, min_count=1, workers=7,epochs=1,negative=0,hs=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "model.train(total_words=len(vocab),total_examples=len(uniqs),corpus_iterable=documents2,epochs=5,report_delay=1)\n",
    "\n",
    "model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thowe\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "q1_word_vec_mean = X_train['question1_tokens'].apply(lambda xs: np.mean([model.wv[x] for x in xs ],axis=0))\n",
    "q2_word_vec_mean = X_train['question2_tokens'].apply(lambda xs: np.mean([model.wv[x] for x in xs ],axis=0))\n",
    "# q1_word_vec_mean = X_train['question1_tokens'].apply(lambda xs: np.mean([model.wv[x] for x in xs if x in keys],axis=0))\n",
    "# q2_word_vec_mean = X_train['question2_tokens'].apply(lambda xs: np.mean([model.wv[x] for x in xs if x in keys],axis=0))\n",
    "X_train['mean_w2v_cos_similarity'] = [cosine_sim(a,b) for a, b in zip(q1_word_vec_mean,q2_word_vec_mean)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# q1_word_vec_sum = X_train['question1_tokens'].apply(lambda xs: np.sum([embed_dict[x] for x in xs if x in keys],axis=0))\n",
    "# q2_word_vec_sum = X_train['question2_tokens'].apply(lambda xs: np.sum([embed_dict[x] for x in xs if x in keys],axis=0))\n",
    "\n",
    "q1_word_vec_sum = X_train['question1_tokens'].apply(lambda xs: np.sum([model.wv[x] for x in xs],axis=0))\n",
    "q2_word_vec_sum = X_train['question2_tokens'].apply(lambda xs: np.sum([model.wv[x] for x in xs],axis=0))\n",
    "X_train['difference_in_word_vec_sum'] = q1_word_vec_sum - q2_word_vec_sum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "          qid1                                   question1_tokens    qid2  \\\nid                                                                          \n20369    38434                             [3, main, compon, cpu]   38435   \n128211  206240    [posit, neg, implic, could, come, time, travel]  206241   \n207728  311500     [feel, see, time, 9:11, everytim, look, clock]  311501   \n327614   14244           [one, correct, lunch, lunch, ate, lunch]   34902   \n206680  310177                                      [read, quora]  120989   \n...        ...                                                ...     ...   \n363276  227992  [place, india, everi, indian, visit, least, life]  385352   \n308849   26089                                [live, happi, life]  260208   \n151699  238470                      [factori, reset, ipod, touch]  238471   \n328037  172373                     [canon, lens, get, 750d, 760d]  454544   \n254671   61668           [common, trait, highli, intellig, peopl]  369445   \n\n                                         question2_tokens  \\\nid                                                          \n20369                                 [main, compon, cpu]   \n128211                           [dark, upper, lip, turn]   \n207728                   [mean, alway, see, 20:20, clock]   \n327614                               [determin, use, etc]   \n206680                                [peopl, use, quora]   \n...                                                   ...   \n363276               [place, everyon, visit, india, life]   \n308849       [person, live, success, happi, life, invest]   \n151699             [perform, factori, reset, ipod, touch]   \n328037  [take, good, photo, use, canon, eo, 750d, also...   \n254671  [peopl, high, intellig, look, peopl, normal, i...   \n\n        tfidf_cos_distances  difference_in_number_of_words  \\\nid                                                           \n20369              0.000000                              1   \n128211             1.000000                              3   \n207728             0.942349                              2   \n327614             1.000000                              3   \n206680             0.885291                             -1   \n...                     ...                            ...   \n363276             0.919887                              2   \n308849             0.760763                             -3   \n151699             0.151386                             -1   \n328037             0.870352                            -12   \n254671             0.911146                             -2   \n\n        mean_w2v_cos_similarity  \\\nid                                \n20369                  0.930678   \n128211                -0.026612   \n207728                 0.852050   \n327614                -0.045885   \n206680                 0.795302   \n...                         ...   \n363276                 0.875294   \n308849                 0.889151   \n151699                 0.990514   \n328037                 0.857252   \n254671                 0.814608   \n\n                               difference_in_word_vec_sum  \nid                                                         \n20369   [-0.18579197, -0.01221776, -1.1425025, 0.98986...  \n128211  [-3.1469753, 2.6548557, -2.0853314, -2.4852917...  \n207728  [0.47789574, 0.78620577, -1.2565527, 1.482996,...  \n327614  [-3.601318, -1.6139027, -1.3311422, 5.542433, ...  \n206680  [-0.4451316, 0.73172224, -0.43761182, 1.614400...  \n...                                                   ...  \n363276  [0.92065156, -1.1661702, -0.27130628, 0.325758...  \n308849  [-0.30834413, 0.8834605, 1.9688396, -0.1131212...  \n151699  [0.46983886, -0.538404, 0.13242865, 0.27794337...  \n328037  [-2.7717135, -1.4312309, 2.711705, -1.5905944,...  \n254671  [0.06972939, -0.6185402, 0.42434537, -2.661664...  \n\n[303215 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid1</th>\n      <th>question1_tokens</th>\n      <th>qid2</th>\n      <th>question2_tokens</th>\n      <th>tfidf_cos_distances</th>\n      <th>difference_in_number_of_words</th>\n      <th>mean_w2v_cos_similarity</th>\n      <th>difference_in_word_vec_sum</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20369</th>\n      <td>38434</td>\n      <td>[3, main, compon, cpu]</td>\n      <td>38435</td>\n      <td>[main, compon, cpu]</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>0.930678</td>\n      <td>[-0.18579197, -0.01221776, -1.1425025, 0.98986...</td>\n    </tr>\n    <tr>\n      <th>128211</th>\n      <td>206240</td>\n      <td>[posit, neg, implic, could, come, time, travel]</td>\n      <td>206241</td>\n      <td>[dark, upper, lip, turn]</td>\n      <td>1.000000</td>\n      <td>3</td>\n      <td>-0.026612</td>\n      <td>[-3.1469753, 2.6548557, -2.0853314, -2.4852917...</td>\n    </tr>\n    <tr>\n      <th>207728</th>\n      <td>311500</td>\n      <td>[feel, see, time, 9:11, everytim, look, clock]</td>\n      <td>311501</td>\n      <td>[mean, alway, see, 20:20, clock]</td>\n      <td>0.942349</td>\n      <td>2</td>\n      <td>0.852050</td>\n      <td>[0.47789574, 0.78620577, -1.2565527, 1.482996,...</td>\n    </tr>\n    <tr>\n      <th>327614</th>\n      <td>14244</td>\n      <td>[one, correct, lunch, lunch, ate, lunch]</td>\n      <td>34902</td>\n      <td>[determin, use, etc]</td>\n      <td>1.000000</td>\n      <td>3</td>\n      <td>-0.045885</td>\n      <td>[-3.601318, -1.6139027, -1.3311422, 5.542433, ...</td>\n    </tr>\n    <tr>\n      <th>206680</th>\n      <td>310177</td>\n      <td>[read, quora]</td>\n      <td>120989</td>\n      <td>[peopl, use, quora]</td>\n      <td>0.885291</td>\n      <td>-1</td>\n      <td>0.795302</td>\n      <td>[-0.4451316, 0.73172224, -0.43761182, 1.614400...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>363276</th>\n      <td>227992</td>\n      <td>[place, india, everi, indian, visit, least, life]</td>\n      <td>385352</td>\n      <td>[place, everyon, visit, india, life]</td>\n      <td>0.919887</td>\n      <td>2</td>\n      <td>0.875294</td>\n      <td>[0.92065156, -1.1661702, -0.27130628, 0.325758...</td>\n    </tr>\n    <tr>\n      <th>308849</th>\n      <td>26089</td>\n      <td>[live, happi, life]</td>\n      <td>260208</td>\n      <td>[person, live, success, happi, life, invest]</td>\n      <td>0.760763</td>\n      <td>-3</td>\n      <td>0.889151</td>\n      <td>[-0.30834413, 0.8834605, 1.9688396, -0.1131212...</td>\n    </tr>\n    <tr>\n      <th>151699</th>\n      <td>238470</td>\n      <td>[factori, reset, ipod, touch]</td>\n      <td>238471</td>\n      <td>[perform, factori, reset, ipod, touch]</td>\n      <td>0.151386</td>\n      <td>-1</td>\n      <td>0.990514</td>\n      <td>[0.46983886, -0.538404, 0.13242865, 0.27794337...</td>\n    </tr>\n    <tr>\n      <th>328037</th>\n      <td>172373</td>\n      <td>[canon, lens, get, 750d, 760d]</td>\n      <td>454544</td>\n      <td>[take, good, photo, use, canon, eo, 750d, also...</td>\n      <td>0.870352</td>\n      <td>-12</td>\n      <td>0.857252</td>\n      <td>[-2.7717135, -1.4312309, 2.711705, -1.5905944,...</td>\n    </tr>\n    <tr>\n      <th>254671</th>\n      <td>61668</td>\n      <td>[common, trait, highli, intellig, peopl]</td>\n      <td>369445</td>\n      <td>[peopl, high, intellig, look, peopl, normal, i...</td>\n      <td>0.911146</td>\n      <td>-2</td>\n      <td>0.814608</td>\n      <td>[0.06972939, -0.6185402, 0.42434537, -2.661664...</td>\n    </tr>\n  </tbody>\n</table>\n<p>303215 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "X_train['number_of_shared_tokens'] = [len(intersection(x,y)) for x,y in zip(X_train['question1_tokens'],X_train['question2_tokens'])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "          qid1                                   question1_tokens    qid2  \\\nid                                                                          \n20369    38434                             [3, main, compon, cpu]   38435   \n128211  206240    [posit, neg, implic, could, come, time, travel]  206241   \n207728  311500     [feel, see, time, 9:11, everytim, look, clock]  311501   \n327614   14244           [one, correct, lunch, lunch, ate, lunch]   34902   \n206680  310177                                      [read, quora]  120989   \n...        ...                                                ...     ...   \n363276  227992  [place, india, everi, indian, visit, least, life]  385352   \n308849   26089                                [live, happi, life]  260208   \n151699  238470                      [factori, reset, ipod, touch]  238471   \n328037  172373                     [canon, lens, get, 750d, 760d]  454544   \n254671   61668           [common, trait, highli, intellig, peopl]  369445   \n\n                                         question2_tokens  \\\nid                                                          \n20369                                 [main, compon, cpu]   \n128211                           [dark, upper, lip, turn]   \n207728                   [mean, alway, see, 20:20, clock]   \n327614                               [determin, use, etc]   \n206680                                [peopl, use, quora]   \n...                                                   ...   \n363276               [place, everyon, visit, india, life]   \n308849       [person, live, success, happi, life, invest]   \n151699             [perform, factori, reset, ipod, touch]   \n328037  [take, good, photo, use, canon, eo, 750d, also...   \n254671  [peopl, high, intellig, look, peopl, normal, i...   \n\n        tfidf_cos_distances  difference_in_number_of_words  \\\nid                                                           \n20369              0.000000                              1   \n128211             1.000000                              3   \n207728             0.942349                              2   \n327614             1.000000                              3   \n206680             0.885291                             -1   \n...                     ...                            ...   \n363276             0.919887                              2   \n308849             0.760763                             -3   \n151699             0.151386                             -1   \n328037             0.870352                            -12   \n254671             0.911146                             -2   \n\n        mean_w2v_cos_similarity  \\\nid                                \n20369                  0.930678   \n128211                -0.026612   \n207728                 0.852050   \n327614                -0.045885   \n206680                 0.795302   \n...                         ...   \n363276                 0.875294   \n308849                 0.889151   \n151699                 0.990514   \n328037                 0.857252   \n254671                 0.814608   \n\n                               difference_in_word_vec_sum  \\\nid                                                          \n20369   [-0.18579197, -0.01221776, -1.1425025, 0.98986...   \n128211  [-3.1469753, 2.6548557, -2.0853314, -2.4852917...   \n207728  [0.47789574, 0.78620577, -1.2565527, 1.482996,...   \n327614  [-3.601318, -1.6139027, -1.3311422, 5.542433, ...   \n206680  [-0.4451316, 0.73172224, -0.43761182, 1.614400...   \n...                                                   ...   \n363276  [0.92065156, -1.1661702, -0.27130628, 0.325758...   \n308849  [-0.30834413, 0.8834605, 1.9688396, -0.1131212...   \n151699  [0.46983886, -0.538404, 0.13242865, 0.27794337...   \n328037  [-2.7717135, -1.4312309, 2.711705, -1.5905944,...   \n254671  [0.06972939, -0.6185402, 0.42434537, -2.661664...   \n\n        number_of_shared_tokens  \nid                               \n20369                         3  \n128211                        0  \n207728                        2  \n327614                        0  \n206680                        1  \n...                         ...  \n363276                        4  \n308849                        3  \n151699                        4  \n328037                        3  \n254671                        2  \n\n[303215 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid1</th>\n      <th>question1_tokens</th>\n      <th>qid2</th>\n      <th>question2_tokens</th>\n      <th>tfidf_cos_distances</th>\n      <th>difference_in_number_of_words</th>\n      <th>mean_w2v_cos_similarity</th>\n      <th>difference_in_word_vec_sum</th>\n      <th>number_of_shared_tokens</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20369</th>\n      <td>38434</td>\n      <td>[3, main, compon, cpu]</td>\n      <td>38435</td>\n      <td>[main, compon, cpu]</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>0.930678</td>\n      <td>[-0.18579197, -0.01221776, -1.1425025, 0.98986...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>128211</th>\n      <td>206240</td>\n      <td>[posit, neg, implic, could, come, time, travel]</td>\n      <td>206241</td>\n      <td>[dark, upper, lip, turn]</td>\n      <td>1.000000</td>\n      <td>3</td>\n      <td>-0.026612</td>\n      <td>[-3.1469753, 2.6548557, -2.0853314, -2.4852917...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>207728</th>\n      <td>311500</td>\n      <td>[feel, see, time, 9:11, everytim, look, clock]</td>\n      <td>311501</td>\n      <td>[mean, alway, see, 20:20, clock]</td>\n      <td>0.942349</td>\n      <td>2</td>\n      <td>0.852050</td>\n      <td>[0.47789574, 0.78620577, -1.2565527, 1.482996,...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>327614</th>\n      <td>14244</td>\n      <td>[one, correct, lunch, lunch, ate, lunch]</td>\n      <td>34902</td>\n      <td>[determin, use, etc]</td>\n      <td>1.000000</td>\n      <td>3</td>\n      <td>-0.045885</td>\n      <td>[-3.601318, -1.6139027, -1.3311422, 5.542433, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>206680</th>\n      <td>310177</td>\n      <td>[read, quora]</td>\n      <td>120989</td>\n      <td>[peopl, use, quora]</td>\n      <td>0.885291</td>\n      <td>-1</td>\n      <td>0.795302</td>\n      <td>[-0.4451316, 0.73172224, -0.43761182, 1.614400...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>363276</th>\n      <td>227992</td>\n      <td>[place, india, everi, indian, visit, least, life]</td>\n      <td>385352</td>\n      <td>[place, everyon, visit, india, life]</td>\n      <td>0.919887</td>\n      <td>2</td>\n      <td>0.875294</td>\n      <td>[0.92065156, -1.1661702, -0.27130628, 0.325758...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>308849</th>\n      <td>26089</td>\n      <td>[live, happi, life]</td>\n      <td>260208</td>\n      <td>[person, live, success, happi, life, invest]</td>\n      <td>0.760763</td>\n      <td>-3</td>\n      <td>0.889151</td>\n      <td>[-0.30834413, 0.8834605, 1.9688396, -0.1131212...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>151699</th>\n      <td>238470</td>\n      <td>[factori, reset, ipod, touch]</td>\n      <td>238471</td>\n      <td>[perform, factori, reset, ipod, touch]</td>\n      <td>0.151386</td>\n      <td>-1</td>\n      <td>0.990514</td>\n      <td>[0.46983886, -0.538404, 0.13242865, 0.27794337...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>328037</th>\n      <td>172373</td>\n      <td>[canon, lens, get, 750d, 760d]</td>\n      <td>454544</td>\n      <td>[take, good, photo, use, canon, eo, 750d, also...</td>\n      <td>0.870352</td>\n      <td>-12</td>\n      <td>0.857252</td>\n      <td>[-2.7717135, -1.4312309, 2.711705, -1.5905944,...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>254671</th>\n      <td>61668</td>\n      <td>[common, trait, highli, intellig, peopl]</td>\n      <td>369445</td>\n      <td>[peopl, high, intellig, look, peopl, normal, i...</td>\n      <td>0.911146</td>\n      <td>-2</td>\n      <td>0.814608</td>\n      <td>[0.06972939, -0.6185402, 0.42434537, -2.661664...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>303215 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "X_train_f = X_train.drop(['qid1','qid2','question1_tokens','question2_tokens'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "        tfidf_cos_distances  difference_in_number_of_words  \\\nid                                                           \n20369              0.000000                              1   \n128211             1.000000                              3   \n207728             0.942349                              2   \n327614             1.000000                              3   \n206680             0.885291                             -1   \n...                     ...                            ...   \n363276             0.919887                              2   \n308849             0.760763                             -3   \n151699             0.151386                             -1   \n328037             0.870352                            -12   \n254671             0.911146                             -2   \n\n        mean_w2v_cos_similarity  difference_in_word_vec_sum  \\\nid                                                            \n20369                  0.930678                   -1.554888   \n128211                -0.026612                   23.649868   \n207728                 0.852050                    9.222132   \n327614                -0.045885                   21.571808   \n206680                 0.795302                    6.991099   \n...                         ...                         ...   \n363276                 0.875294                    0.201715   \n308849                 0.889151                   -1.385588   \n151699                 0.990514                    2.082810   \n328037                 0.857252                   -5.359364   \n254671                 0.814608                  -16.713158   \n\n        number_of_shared_tokens  \nid                               \n20369                         3  \n128211                        0  \n207728                        2  \n327614                        0  \n206680                        1  \n...                         ...  \n363276                        4  \n308849                        3  \n151699                        4  \n328037                        3  \n254671                        2  \n\n[303215 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tfidf_cos_distances</th>\n      <th>difference_in_number_of_words</th>\n      <th>mean_w2v_cos_similarity</th>\n      <th>difference_in_word_vec_sum</th>\n      <th>number_of_shared_tokens</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20369</th>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>0.930678</td>\n      <td>-1.554888</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>128211</th>\n      <td>1.000000</td>\n      <td>3</td>\n      <td>-0.026612</td>\n      <td>23.649868</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>207728</th>\n      <td>0.942349</td>\n      <td>2</td>\n      <td>0.852050</td>\n      <td>9.222132</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>327614</th>\n      <td>1.000000</td>\n      <td>3</td>\n      <td>-0.045885</td>\n      <td>21.571808</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>206680</th>\n      <td>0.885291</td>\n      <td>-1</td>\n      <td>0.795302</td>\n      <td>6.991099</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>363276</th>\n      <td>0.919887</td>\n      <td>2</td>\n      <td>0.875294</td>\n      <td>0.201715</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>308849</th>\n      <td>0.760763</td>\n      <td>-3</td>\n      <td>0.889151</td>\n      <td>-1.385588</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>151699</th>\n      <td>0.151386</td>\n      <td>-1</td>\n      <td>0.990514</td>\n      <td>2.082810</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>328037</th>\n      <td>0.870352</td>\n      <td>-12</td>\n      <td>0.857252</td>\n      <td>-5.359364</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>254671</th>\n      <td>0.911146</td>\n      <td>-2</td>\n      <td>0.814608</td>\n      <td>-16.713158</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>303215 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_f['difference_in_word_vec_sum'] = X_train_f['difference_in_word_vec_sum'].apply(np.sum)\n",
    "X_train_f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Modeling\n",
    "\n",
    "Different modeling techniques can be used:\n",
    "\n",
    "- logistic regression\n",
    "- XGBoost\n",
    "- LSTMs\n",
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(random_state=1738,subsample=0.7,colsample_bytree=0.8,tree_method='hist',reg_lambda=2,alpha=.5,max_depth=10,n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "XGBClassifier(alpha=0.5, base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n              importance_type=None, interaction_constraints='',\n              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=10, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=500,\n              n_jobs=0, num_parallel_tree=1, predictor='auto',\n              random_state=1738, reg_alpha=0.5, ...)"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_f,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.8662005507643091"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train_f,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6316027566027566"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, roc_auc_score, plot_roc_curve, accuracy_score, recall_score\n",
    "\n",
    "recall_score(clf.predict(X_train_f),y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Doc2VecTransformer(TransformerMixin,BaseEstimator):\n",
    "    def __init__(self,uniqs=None):\n",
    "        self.d2v = None\n",
    "        self.uniqs = uniqs\n",
    "\n",
    "    def fit(self,X, y=None):\n",
    "        if self.uniqs is None:\n",
    "            self.uniqs = get_uniqs(X)\n",
    "        documents2 = [TaggedDocument(*doc, tags=[i]) for i, doc in self.uniqs.iterrows()]\n",
    "        self.d2v = Doc2Vec(documents2, vector_size=50, window=5, min_count=1, workers=7,epochs=1,negative=0,hs=1)\n",
    "        self.d2v.train(total_words=len(get_vocab(self.uniqs)),total_examples=len(self.uniqs),corpus_iterable=documents2,epochs=5,report_delay=1)\n",
    "\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        q1_word_vec_mean = X['question1_tokens'].apply(lambda xs: np.mean([self.d2v.wv[x] for x in xs if ],axis=0))\n",
    "        q2_word_vec_mean = X_train['question2_tokens'].apply(lambda xs: np.mean([self.d2v.wv[x] for x in xs ],axis=0))\n",
    "        X['mean_w2v_cos_similarity'] = [cosine_sim(a,b) for a, b in zip(q1_word_vec_mean,q2_word_vec_mean)]\n",
    "\n",
    "        q1_word_vec_sum = X['question1_tokens'].apply(lambda xs: np.sum([self.d2v.wv[x] for x in xs],axis=0))\n",
    "        q2_word_vec_sum = X['question2_tokens'].apply(lambda xs: np.sum([self.d2v.wv[x] for x in xs],axis=0))\n",
    "        X['difference_in_word_vec_sum'] = q1_word_vec_sum - q2_word_vec_sum\n",
    "\n",
    "\n",
    "class SharedWordCounter(TransformerMixin,BaseEstimator):\n",
    "    def fit(self,X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X['number_of_shared_tokens'] = [len(intersection(x,y)) for x,y in zip(X['question1_tokens'],X['question2_tokens'])]\n",
    "\n",
    "\n",
    "class DifferenceInNumberOfWordsTransformer(TransformerMixin,BaseEstimator):\n",
    "    def fit(self,X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        q1_lens = X['question1_tokens'].apply(len)\n",
    "        q2_lens = X['question2_tokens'].apply(len)\n",
    "        X['difference_in_number_of_words'] = q1_lens - q2_lens\n",
    "        return X\n",
    "\n",
    "\n",
    "class Uniquer(TransformerMixin,BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.uniqs = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        q1_ids = X[['qid1','question1_tokens']].groupby('qid1').first()\n",
    "        q1_ids = q1_ids.reset_index()\n",
    "\n",
    "        q1_ids.columns = ['qid','tokens']\n",
    "\n",
    "        q2_ids = X[['qid2','question2_tokens']].groupby('qid2').first()\n",
    "        q2_ids = q2_ids.reset_index()\n",
    "        q2_ids.columns = ['qid','tokens']\n",
    "        self.uniqs = pd.concat([q1_ids,q2_ids],axis=0).groupby('qid').first()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_uniqs(X):\n",
    "    q1_ids = X[['qid1','question1_tokens']].groupby('qid1').first()\n",
    "    q1_ids = q1_ids.reset_index()\n",
    "    q1_ids.columns = ['qid','tokens']\n",
    "\n",
    "    q2_ids = X[['qid2','question2_tokens']].groupby('qid2').first()\n",
    "    q2_ids = q2_ids.reset_index()\n",
    "    q2_ids.columns = ['qid','tokens']\n",
    "    uniqs = pd.concat([q1_ids,q2_ids],axis=0).groupby('qid').first()\n",
    "\n",
    "    return uniqs\n",
    "\n",
    "def get_vocab(uniqs):\n",
    "    vocab = np.concatenate(list(uniqs['tokens']))\n",
    "    vocab = list(set(vocab))\n",
    "\n",
    "    return vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "d2v = Doc2VecTransformer()\n",
    "swc = SharedWordCounter()\n",
    "dinowt = DifferenceInNumberOfWordsTransformer()\n",
    "vec_tfidf\n",
    "fun = FeatureUnion([('d2v',d2v),('swc',swc),('dinowt',dinowt),('vec_tfidf',vec_tfidf)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [87]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m fun \u001B[38;5;241m=\u001B[39m \u001B[43mfun\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\sklearn\\pipeline.py:1143\u001B[0m, in \u001B[0;36mFeatureUnion.fit\u001B[1;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[0;32m   1124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params):\n\u001B[0;32m   1125\u001B[0m     \u001B[38;5;124;03m\"\"\"Fit all transformers using X.\u001B[39;00m\n\u001B[0;32m   1126\u001B[0m \n\u001B[0;32m   1127\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1141\u001B[0m \u001B[38;5;124;03m        FeatureUnion class instance.\u001B[39;00m\n\u001B[0;32m   1142\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1143\u001B[0m     transformers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parallel_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_fit_one\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1144\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m transformers:\n\u001B[0;32m   1145\u001B[0m         \u001B[38;5;66;03m# All transformers are None\u001B[39;00m\n\u001B[0;32m   1146\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\sklearn\\pipeline.py:1194\u001B[0m, in \u001B[0;36mFeatureUnion._parallel_func\u001B[1;34m(self, X, y, fit_params, func)\u001B[0m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_transformer_weights()\n\u001B[0;32m   1192\u001B[0m transformers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter())\n\u001B[1;32m-> 1194\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1195\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1196\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1198\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1199\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1200\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessage_clsname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mFeatureUnion\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1201\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_log_message\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtransformers\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1202\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1203\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1204\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtransformers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1205\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\joblib\\parallel.py:1046\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1043\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[0;32m   1044\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1046\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1047\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pre_dispatch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1050\u001B[0m     \u001B[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001B[39;00m\n\u001B[0;32m   1051\u001B[0m     \u001B[38;5;66;03m# No need to wait for async callbacks to trigger to\u001B[39;00m\n\u001B[0;32m   1052\u001B[0m     \u001B[38;5;66;03m# consumption.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\joblib\\parallel.py:861\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    860\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 861\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    862\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\joblib\\parallel.py:779\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    777\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    778\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[1;32m--> 779\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    780\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[0;32m    781\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[0;32m    782\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[0;32m    784\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[1;34m(self, func, callback)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mImmediateResult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[0;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    569\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[0;32m    570\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[1;32m--> 572\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\joblib\\parallel.py:262\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\joblib\\parallel.py:262\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig):\n\u001B[1;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\sklearn\\pipeline.py:907\u001B[0m, in \u001B[0;36m_fit_one\u001B[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001B[0m\n\u001B[0;32m    903\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    904\u001B[0m \u001B[38;5;124;03mFits ``transformer`` to ``X`` and ``y``.\u001B[39;00m\n\u001B[0;32m    905\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    906\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(message_clsname, message):\n\u001B[1;32m--> 907\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m transformer\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2053\u001B[0m, in \u001B[0;36mTfidfVectorizer.fit\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   2051\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params()\n\u001B[0;32m   2052\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_warn_for_unused_params()\n\u001B[1;32m-> 2053\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2054\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mfit(X)\n\u001B[0;32m   2055\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001B[0m, in \u001B[0;36mCountVectorizer.fit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   1322\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1323\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpper case characters found in\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1324\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m vocabulary while \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlowercase\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1325\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is True. These entries will not\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1326\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be matched with any documents\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1327\u001B[0m             )\n\u001B[0;32m   1328\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m-> 1330\u001B[0m vocabulary, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_count_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfixed_vocabulary_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1332\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n\u001B[0;32m   1333\u001B[0m     X\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1220\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[1;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[0;32m   1218\u001B[0m     vocabulary \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(vocabulary)\n\u001B[0;32m   1219\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m vocabulary:\n\u001B[1;32m-> 1220\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1221\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1222\u001B[0m         )\n\u001B[0;32m   1224\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m indptr[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39miinfo(np\u001B[38;5;241m.\u001B[39mint32)\u001B[38;5;241m.\u001B[39mmax:  \u001B[38;5;66;03m# = 2**31 - 1\u001B[39;00m\n\u001B[0;32m   1225\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _IS_32BIT:\n",
      "\u001B[1;31mValueError\u001B[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "fun = fun.fit(X_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'wv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [91]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m X_test_f \u001B[38;5;241m=\u001B[39m \u001B[43mfun\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\sklearn\\pipeline.py:1222\u001B[0m, in \u001B[0;36mFeatureUnion.transform\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m   1207\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m   1208\u001B[0m     \u001B[38;5;124;03m\"\"\"Transform X separately by each transformer, concatenate results.\u001B[39;00m\n\u001B[0;32m   1209\u001B[0m \n\u001B[0;32m   1210\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1220\u001B[0m \u001B[38;5;124;03m        sum of `n_components` (output dimension) over transformers.\u001B[39;00m\n\u001B[0;32m   1221\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1222\u001B[0m     Xs \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1223\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_transform_one\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrans\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1224\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrans\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_iter\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1225\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1226\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m Xs:\n\u001B[0;32m   1227\u001B[0m         \u001B[38;5;66;03m# All transformers are None\u001B[39;00m\n\u001B[0;32m   1228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mzeros((X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m0\u001B[39m))\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\joblib\\parallel.py:1043\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1034\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1035\u001B[0m     \u001B[38;5;66;03m# Only set self._iterating to True if at least a batch\u001B[39;00m\n\u001B[0;32m   1036\u001B[0m     \u001B[38;5;66;03m# was dispatched. In particular this covers the edge\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1040\u001B[0m     \u001B[38;5;66;03m# was very quick and its callback already dispatched all the\u001B[39;00m\n\u001B[0;32m   1041\u001B[0m     \u001B[38;5;66;03m# remaining jobs.\u001B[39;00m\n\u001B[0;32m   1042\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m-> 1043\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1044\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1046\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\joblib\\parallel.py:861\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    860\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 861\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    862\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\joblib\\parallel.py:779\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    777\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    778\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[1;32m--> 779\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    780\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[0;32m    781\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[0;32m    782\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[0;32m    784\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[1;34m(self, func, callback)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mImmediateResult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[0;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    569\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[0;32m    570\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[1;32m--> 572\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\joblib\\parallel.py:262\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\joblib\\parallel.py:262\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig):\n\u001B[1;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\LighthouseEnv\\lib\\site-packages\\sklearn\\pipeline.py:876\u001B[0m, in \u001B[0;36m_transform_one\u001B[1;34m(transformer, X, y, weight, **fit_params)\u001B[0m\n\u001B[0;32m    875\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_transform_one\u001B[39m(transformer, X, y, weight, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params):\n\u001B[1;32m--> 876\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mtransformer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    877\u001B[0m     \u001B[38;5;66;03m# if we have a weight for this transformer, multiply output\u001B[39;00m\n\u001B[0;32m    878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "Input \u001B[1;32mIn [89]\u001B[0m, in \u001B[0;36mDoc2VecTransformer.transform\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m---> 16\u001B[0m     keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43md2v\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwv\u001B[49m\u001B[38;5;241m.\u001B[39mindex_to_keys()\n\u001B[0;32m     17\u001B[0m     q1_word_vec_mean \u001B[38;5;241m=\u001B[39m X[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquestion1_tokens\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m xs: np\u001B[38;5;241m.\u001B[39mmean([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md2v\u001B[38;5;241m.\u001B[39mwv[x] \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m xs \u001B[38;5;28;01mif\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m keys],axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m))\n\u001B[0;32m     18\u001B[0m     q2_word_vec_mean \u001B[38;5;241m=\u001B[39m X_train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquestion2_tokens\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m xs: np\u001B[38;5;241m.\u001B[39mmean([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md2v\u001B[38;5;241m.\u001B[39mwv[x] \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m xs ],axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m))\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'wv'"
     ]
    }
   ],
   "source": [
    "X_test_f = fun.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "lighthouseenv",
   "language": "python",
   "display_name": "LighthouseEnv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}